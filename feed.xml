<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jac2125.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jac2125.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-18T10:20:48+00:00</updated><id>https://jac2125.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">ThoughtsOnCrossProduct</title><link href="https://jac2125.github.io/blog/2025/Thoughts-on-Cross-Product/" rel="alternate" type="text/html" title="ThoughtsOnCrossProduct"/><published>2025-03-01T00:00:00+00:00</published><updated>2025-03-01T00:00:00+00:00</updated><id>https://jac2125.github.io/blog/2025/Thoughts-on-Cross-Product</id><content type="html" xml:base="https://jac2125.github.io/blog/2025/Thoughts-on-Cross-Product/"><![CDATA[<p>This is a mini-post about my thought about the cross product. The definition is the following:</p> <p>Let (\mathbf{a} = \langle a_1, a_2, a_3 \rangle) and (\mathbf{b} = \langle b_1, b_2, b_3 \rangle).<br/> The cross product is defined as:</p> \[\mathbf{a} \times \mathbf{b} = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\ a_1 &amp; a_2 &amp; a_3 \\ b_1 &amp; b_2 &amp; b_3 \\ \end{vmatrix}\] <p>Expanding this determinant:</p> \[\mathbf{a} \times \mathbf{b} = (a_2 b_3 - a_3 b_2)\mathbf{i} - (a_1 b_3 - a_3 b_1)\mathbf{j} + (a_1 b_2 - a_2 b_1)\mathbf{k}\] <p>Or, in vector form:</p> \[\mathbf{a} \times \mathbf{b} = \langle a_2 b_3 - a_3 b_2,\ a_3 b_1 - a_1 b_3,\ a_1 b_2 - a_2 b_1 \rangle\] <p>They said, the length of vector given by the cross product of two vector is equivalent to the area of parellogram made by those two vectors.</p> <div class="row justify-content-sm-center"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Cross_product-480.webp 480w,/assets/img/Cross_product-800.webp 800w,/assets/img/Cross_product-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Cross_product.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Cross Product" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>But why the length is really equal to the area of the parellogram? I was wondering this from my highschool. For someone who’s studying cross product and wondering why they are equal to each other, this will help.</p> <p>To begin with, you must know the <strong>determinant</strong> of the matrix.</p> <p>Let’s see this example: \(\begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\ 3 &amp; 4 &amp; 0 \\ 1 &amp; 2 &amp; 0 \\ \end{vmatrix}\)</p> <p>Those two vectors are in 2-Dimensional plane as they do not have the third element. The definition of the cross product would give: \((0)\mathbf{i} - (0)\mathbf{j} + (2*3-1*4)\mathbf{k} = \begin{vmatrix} 3 &amp; 4 \\ 1 &amp; 2 \\ \end{vmatrix} \mathbf{k}\)</p> <p>Then we need to show why the determinant is equal to the volume. First we need to assume that the rows are linearly independent, unless the determinant would be zero. Adding the multiple of a column to another does not change the determinant. So, by doing some ERO (Eelementary Row Operation), you would end up a diagonal matrix whose determinant is the same as the original matrix. Think each diagonal entry as a side length of rectangle (e.g. in 2D plane). So the determinant is really the volume made by columns of matrix.</p> <p>Back to where we started, after getting a area, we multiplied this area to \(\mathbf{k}\)</p>]]></content><author><name></name></author><category term="math"/><category term="math"/><category term="LinearAlgebra"/><summary type="html"><![CDATA[This is a mini-post about my thought about the cross product. The definition is the following:]]></summary></entry><entry><title type="html">TSP(s)</title><link href="https://jac2125.github.io/blog/2025/TSP-Problems/" rel="alternate" type="text/html" title="TSP(s)"/><published>2025-02-25T00:00:00+00:00</published><updated>2025-02-25T00:00:00+00:00</updated><id>https://jac2125.github.io/blog/2025/TSP-Problems</id><content type="html" xml:base="https://jac2125.github.io/blog/2025/TSP-Problems/"><![CDATA[<h1 id="a-brief-intro-to-the-tsp">A Brief Intro to the TSP</h1> <p><em>A Gentle Intro Optimization</em> by B. Guenin, J. Konemann and L. Tuncel has introduced the TSP briefly. <br/> Consider a traveling salesman who needs to visit cities \(1, 2, 3, ..., n\) in some order and end up at the city where he started. Cost of travel from city \(i\) to city \(j\) is given by \(c_{ij}\). The goal is to find a <em>tour</em> of these \(n\) cities, visiting city exactly once, such that the total travel costs are minimized. \ <br/> The tour in this problem refers to a Hamiltonian cycle. And it is NP-complete to decide whether a given undirected graph \(G = (V, E)\) has a Hamiltonian Cycle.</p>]]></content><author><name></name></author><category term="math"/><category term="math"/><category term="TSP"/><summary type="html"><![CDATA[A Brief Intro to the TSP A Gentle Intro Optimization by B. Guenin, J. Konemann and L. Tuncel has introduced the TSP briefly. Consider a traveling salesman who needs to visit cities \(1, 2, 3, ..., n\) in some order and end up at the city where he started. Cost of travel from city \(i\) to city \(j\) is given by \(c_{ij}\). The goal is to find a tour of these \(n\) cities, visiting city exactly once, such that the total travel costs are minimized. \ The tour in this problem refers to a Hamiltonian cycle. And it is NP-complete to decide whether a given undirected graph \(G = (V, E)\) has a Hamiltonian Cycle.]]></summary></entry><entry><title type="html">Circulant Matrix + Eigenvectors/values</title><link href="https://jac2125.github.io/blog/2024/Circulant-Matrix-and-Eigenvectors&values/" rel="alternate" type="text/html" title="Circulant Matrix + Eigenvectors/values"/><published>2024-12-15T15:12:00+00:00</published><updated>2024-12-15T15:12:00+00:00</updated><id>https://jac2125.github.io/blog/2024/Circulant-Matrix-and-Eigenvectors&amp;values</id><content type="html" xml:base="https://jac2125.github.io/blog/2024/Circulant-Matrix-and-Eigenvectors&amp;values/"><![CDATA[<h1 id="1-problem">1. Problem</h1> <p>![[Pasted image 20241029213125.png]] Here is the proposition regarding the eigenvalues of Laplacian Matrix $C_n$ While reading the proof of this proposition, I have encoutered to a question of the following: ![[Pasted image 20241029213250.png]] the equation 4.6 to 4.8 is pretty straight forward, but it took me some time to understand this part: <strong>If $P$ is a cyclic permutation, then $P\vec{x}$ is also an eigenvector for $\lambda$. This means that $\vec{x}, P\vec{x},…,P^{n-1} \vec{x}$</strong> are all eigenvectors of $\lambda$ I did some of the research what the cyclic permutation is meaning and studying about topics related circulant matrix, which was also very interesting. So I wanted to write my own proof about this but it is not clear if this is really a “proof” (I am not sure if this proof can be a rigorous proof on this)</p> <p>Proposition: Let $\vec{x}$ be an eigenvector of $L_{C_n}$. If $P$ is a cyclic permutation, then $P\vec{x}$ is also an eigenvector for $\lambda$. This means that $\vec{x}, P\vec{x},…,P^{n-1} \vec{x}$</p> <p>The laplacian matrix of $C_n$ has the form \(\begin{bmatrix}2 &amp; -1 &amp; 0 &amp; ... &amp; -1 \\ -1 &amp; 2 &amp; -1 &amp; ... &amp; 0\\ \vdots &amp; &amp; \ddots &amp; &amp; \vdots\\ 0 &amp; ... &amp; -1 &amp; 2 &amp; -1\\ -1 &amp; ... &amp; 0 &amp; -1 &amp; 2\\ \end{bmatrix}\) Then</p> <p>I realized it: next we study <strong>fourier analysis</strong> to understand the rest of the proofs</p>]]></content><author><name></name></author><category term="math"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[1. Problem ![[Pasted image 20241029213125.png]] Here is the proposition regarding the eigenvalues of Laplacian Matrix $C_n$ While reading the proof of this proposition, I have encoutered to a question of the following: ![[Pasted image 20241029213250.png]] the equation 4.6 to 4.8 is pretty straight forward, but it took me some time to understand this part: If $P$ is a cyclic permutation, then $P\vec{x}$ is also an eigenvector for $\lambda$. This means that $\vec{x}, P\vec{x},…,P^{n-1} \vec{x}$ are all eigenvectors of $\lambda$ I did some of the research what the cyclic permutation is meaning and studying about topics related circulant matrix, which was also very interesting. So I wanted to write my own proof about this but it is not clear if this is really a “proof” (I am not sure if this proof can be a rigorous proof on this)]]></summary></entry><entry><title type="html">Laplacian Matrix - Basic Properties</title><link href="https://jac2125.github.io/blog/2024/Laplacian-Matrix-(Basic-Properties)/" rel="alternate" type="text/html" title="Laplacian Matrix - Basic Properties"/><published>2024-11-10T15:12:00+00:00</published><updated>2024-11-10T15:12:00+00:00</updated><id>https://jac2125.github.io/blog/2024/Laplacian-Matrix-(Basic-Properties)</id><content type="html" xml:base="https://jac2125.github.io/blog/2024/Laplacian-Matrix-(Basic-Properties)/"><![CDATA[<p>This page is my study note of AN INTRODUCTION TO SPECTRAL GRAPH THEORY by Jiaqi Jiang</p> <p>From Wikipedia, it introduces the spectral theory as the following: In mathematics, spectral theory is an inclusive term for theories extending the eigenvector and eigenvalue theory of a single square matrix to a much broader theory of the structure of operators in a variety of mathematical spaces.</p> <p>Then Spectral Graph Theory is a study of Eigenvalues/Eigenvectors of Graph. It deals with matrix representations of graphs, such as Laplacian and Adjacency Matrix. In this page, I wrote some cool properties of Laplacian Matrix and will write further study note about it. Alright, before we hit the definition of Laplacian Matrix, here is the definition of \(L_{G_{\{u,v\}}}\).</p> <h2 id="definition-1">Definition 1</h2> <p>Suppose \(G = (V, E)\) is a graph with \(V = \{1, 2, \dots, n\}\). For an edge \(\{u, v\} \in E\), we define an \(n \times n\) matrix \(L_{G_{\{u,v\}}}\) by</p> \[l_{G_{(u,v)}}(i,j) = \begin{cases} 1 &amp; \text{if } i = j \text{ and } i \in \{u, v\}, \\ -1 &amp; \text{if } i = u \text{ and } j = v, \text{ or vice versa}, \\ 0 &amp; \text{otherwise}. \end{cases}\] <p>The matrix \(L_{G_{\{u,v\}}}\) has cool properties: \(\vec{x}^T L_{G_{\{u,v\}}} \vec{x} = (x_u - x_v)^2\) for all \(\vec{x} \in \mathbb{R}^n\)</p> <p>Then let’s define the laplacian matrix:</p> <h2 id="definition-2">Definition 2</h2> <p>For a graph \(G = (V, E)\), the Laplacian matrix \(L_G\) is defined as:</p> \[L_G = \sum_{\{u, v\} \in E} L_{G_{\{u, v\}}}\] <p>Alternative definition: Let \(G = (V, E)\) with \(n\) vertices \(v_1, ... v_n\), its Laplacian matrix \(L_{n\times n} is defined as\) L_{i,j}:= \begin{cases<em>} \text{deg}(v_i) &amp; \text{if } i=j, \ -1 &amp; \text{if } i \neq j \and (v_i, v_j) \in E \ 0 &amp; \text{otherwise} \end{cases</em>} This is equivalent to \(L = D - A\), where \(D\) is the degree matrix, and \(A\) is the graph’s adjacency matrix.</p> <p>We can check that this matrix is a real-symmetric matrix, where all the entries are real and symmetric. Here is one good fact about a real-symmetric matrix</p> <h2 id="theorem-1">Theorem 1</h2> <p>The eigenvalues of a self-adjoint matrix are all real.</p> <p><strong>Proof.</strong> Suppose \(\lambda\) is an eigenvalue of the self-adjoint matrix \(L\) and \(v\) is a nonzero eigenvector of \(\lambda\). Then,</p> <p>\(\begin{align*} \lambda \|v\|^2 &amp;= \lambda \langle v, v \rangle \\ &amp;= \langle \lambda v, v \rangle \\ &amp;= \langle Lv, \rangle \\ &amp;= \langle v, Lv \rangle \\ &amp;= \langle v, \lambda v \rangle &amp;= \bar{\lambda} \langle v, v \rangle &amp;= \bar{\lambda} \|v\|^2 \end{align*}\) Since \(\lambda = \bar{\lambda}\), \(\lambda\) is real.</p> <p>I just said “self-adjoint matrix”. A self-disjoint matrix \(M \in \mathbb{F}^{n\times n}\) is self-disjoint or Hemitian if the conjugate transpose is equal to the original matrix: \(M = M^*\) Since real-symmetric matrices are equal to its conjugate transpose, we can apply theorem number to this case as well.</p> <p>And then, this is the definition of positive-semidefinite matrix.</p> <h2 id="definition-34">Definition 3.4</h2> <p>An \(n \times n\) matrix \(M\) is called positive-semidefinite if \(\underline{x}^{T} M \underline{x} \geq 0\) for all \(\underline{x} \in \mathbb{R}^n\).</p> <h2 id="theorem-35">Theorem 3.5</h2> <p>For a graph \(G\), every eigenvalue of \(L_G\) is non-negative.</p> <p><strong>Proof.</strong> Suppose \(\lambda\) is an eigenvalue and \(\mathbf{x} \in \mathbb{R}^n\) is a nonzero eigenvector of \(\lambda\). Then,</p> \[\mathbf{x}^T L_G \mathbf{x} = \mathbf{x}^T (\lambda \mathbf{x}) = \lambda (\mathbf{x}^T \mathbf{x}).\] <p>Since \(\mathbf{x}^T L_G \mathbf{x} &gt; 0\) and \(\mathbf{x}^T \mathbf{x} &gt; 0\), we have \(\lambda &gt; 0\). \(\square\)</p> <p>As we have noted, the Laplacian matrix \(L_G\) is self-adjoint and consists of real entries. Thus, the Real Spectral Theorem states that \(L_G\) has an orthonormal basis consisting of eigenvectors of \(L_G\). Therefore, for a graph \(G\) of \(n\) vertices, we can find \(n\) eigenvalues (not necessarily distinct) for \(L_G\). We denote them as \(\lambda_1, \lambda_2, \ldots, \lambda_n\). Since they are all real and non-negative, we assume that</p> \[0 &lt; \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n.\] <p>Now we prove some fundamental facts about Laplacians. Recall that in the previous section we mentioned that the eigenvalues of the Laplacian tell us how connected a graph is. Now, we define connectedness. First, we give the definition of a <em>path</em>.</p> <h2 id="corollary-310">Corollary 3.10</h2> <p>Let \(G = (V, E)\) be a graph. Then the multiplicity of \(0\) as an eigenvalue of \(L_G\) equals the number of connected components of \(G\).</p> <p><strong>Proof.</strong> Suppose \(G_1 = (V_1, E_1), G_2 = (V_2, E_2), \ldots, G_k = (V_k, E_k)\) are the connected components of \(G\). Let \(\mathbf{w}_i\) be defined by</p> \[(w_i)_{j} = \begin{cases} 1 &amp; \text{if } j \in V_i, \\ 0 &amp; \text{otherwise}. \end{cases}\] <p>Then, it follows from the previous lemma that if \(\mathbf{x} \in \mathbb{R}^n\) is a non-zero eigenvector of \(0\), then \(x_i = x_j\) for any \(i, j \in V\) such that \(i, j\) are in the same connected component. So,</p> \[U_{i} = \text{Span}(\{\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_k\}).\] <p>It is clear that \(\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_k\) are linearly independent. Therefore, the multiplicity of \(0\) as an eigenvalue of \(L_G\) is the number of connected components in \(G\). \(\square\)</p> <h1 id="4-eigenvalues-and-eigenvectors-of-the-laplacians-of-some-fundamental-graphs">4. Eigenvalues and Eigenvectors of the Laplacians of Some Fundamental Graphs</h1> <p>Now we begin to examine the eigenvalues and the eigenvectors of the Laplacian of some fundamental graphs.</p> <h2 id="definition-41">Definition 4.1</h2> <p>A complete graph on \(n\) vertices, \(K_n\), is a graph \(G = (V, E)\) where \(V = \{1, 2, \ldots, n\}\) and \(E = \{(i, j) \mid i \neq j, i, j \in V\}\).</p> <h2 id="proposition-42">Proposition 4.2</h2> <p>The Laplacian of \(K_n\) has eigenvalue \(0\) with multiplicity \(1\) and eigenvalue \(\nu\) with multiplicity \(n - 1\).</p> <p><strong>Proof.</strong> The first part of the proposition simply follows from Corollary 3.9. To prove the second part of the proposition, consider the Laplacian of \(K_n\). It is an \(n \times n\) matrix defined by</p> \[a_{ij} = \begin{cases} -1 &amp; \text{if } i \neq j, \\ n - 1 &amp; \text{if } i = j. \end{cases}\] <p>Therefore, \(L_K - nI = M\) where \(M\) is the \(n \times n\) matrix with entries all equal to \(-1\). Clearly, \(M\) is not invertible and has rank \(1\). Thus \(n\) is an eigenvalue of \(L_K\). Then by Rank-nullity Theorem, \(\text{null}(M) = n - 1\). It follows that the eigenvalue \(\nu\) has multiplicity \(n - 1\). \(\square\)</p> <h2 id="definition-43">Definition 4.3</h2> <p>The path graph on \(n\) vertices, \(P_n\), is a graph \(G = (V, E)\) where \(V = \{1, 2, \ldots, n\}\) and \(E = \{(i, i + 1) \mid 1 \leq i &lt; n\}\).</p>]]></content><author><name></name></author><category term="math"/><category term="Spectral_Graph_Theory"/><summary type="html"><![CDATA[This page is my study note of AN INTRODUCTION TO SPECTRAL GRAPH THEORY by Jiaqi Jiang]]></summary></entry></feed>